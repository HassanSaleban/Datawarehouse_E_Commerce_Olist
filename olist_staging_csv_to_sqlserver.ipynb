{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dee1c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> olist_customers_dataset.csv  -> [staging.customers]\n",
      "    OK (99,441 lignes, chunksize=420)\n",
      "--> olist_geolocation_dataset.csv  -> [staging.geolocation]\n",
      "    OK (1,000,163 lignes, chunksize=420)\n",
      "--> olist_order_items_dataset.csv  -> [staging.order_items]\n",
      "    OK (112,650 lignes, chunksize=300)\n",
      "--> olist_order_payments_dataset.csv  -> [staging.order_payments]\n",
      "    OK (103,886 lignes, chunksize=420)\n",
      "--> olist_order_reviews_dataset.csv  -> [staging.order_reviews]\n",
      "    OK (99,224 lignes, chunksize=300)\n",
      "--> olist_orders_dataset.csv  -> [staging.orders]\n",
      "    OK (99,441 lignes, chunksize=262)\n",
      "--> olist_products_dataset.csv  -> [staging.products]\n",
      "    OK (32,951 lignes, chunksize=233)\n",
      "--> olist_sellers_dataset.csv  -> [staging.sellers]\n",
      "    OK (3,095 lignes, chunksize=525)\n",
      "--> product_category_name_translation.csv  -> [staging.product_category_name_translation]\n",
      "    OK (71 lignes, chunksize=1050)\n",
      "✅ Import terminé dans le schéma 'staging' de la base olist_staging.\n"
     ]
    }
   ],
   "source": [
    "import os, glob, urllib.parse\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, event, text\n",
    "\n",
    "# ========= PARAMÈTRES SQL =========\n",
    "SERVER   = \"LAPTOP-V857ADI3\"            # machine locale\n",
    "DATABASE = \"olist_db_staging\"\n",
    "USER     = \"userairflow\"\n",
    "PWD      = \"air123\"\n",
    "DRIVER   = \"ODBC Driver 17 for SQL Server\"   # tu as le 17 installé\n",
    "SCHEMA   = \"staging\"                         # schéma cible\n",
    "\n",
    "# ========= DOSSIER CSV =========\n",
    "CSV_DIR = r\"C:\\Users\\saleban ali hassan\\Documents\\olist\"\n",
    "\n",
    "# ========= MAPPING nom_fichier -> nom_table =========\n",
    "NAME_MAP = {\n",
    "    \"olist_customers_dataset\": \"customers\",\n",
    "    \"olist_geolocation_dataset\": \"geolocation\",\n",
    "    \"olist_order_items_dataset\": \"order_items\",\n",
    "    \"olist_order_payments_dataset\": \"order_payments\",\n",
    "    \"olist_order_reviews_dataset\": \"order_reviews\",\n",
    "    \"olist_orders_dataset\": \"orders\",\n",
    "    \"olist_products_dataset\": \"products\",\n",
    "    \"olist_sellers_dataset\": \"sellers\",\n",
    "    \"product_category_name_translation\": \"product_category_name_translation\",\n",
    "}\n",
    "\n",
    "# ========= CONNEXION (SQLAlchemy + pyodbc) =========\n",
    "odbc_str = (\n",
    "    f\"DRIVER={{{DRIVER}}};\"\n",
    "    f\"SERVER={SERVER};\"\n",
    "    f\"DATABASE={DATABASE};\"\n",
    "    f\"UID={USER};PWD={PWD};\"\n",
    "    \"Encrypt=yes;TrustServerCertificate=yes;\"\n",
    ")\n",
    "params = urllib.parse.quote_plus(odbc_str)\n",
    "engine = create_engine(f\"mssql+pyodbc:///?odbc_connect={params}\")\n",
    "\n",
    "# Accélère executemany côté pyodbc\n",
    "@event.listens_for(engine, \"before_cursor_execute\")\n",
    "def _fast_exec(conn, cursor, statement, parameters, context, executemany):\n",
    "    if executemany:\n",
    "        try:\n",
    "            cursor.fast_executemany = True\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# ========= Créer le schéma 'staging' s'il n'existe pas =========\n",
    "with engine.begin() as conn:\n",
    "    exists = conn.execute(\n",
    "        text(\"SELECT 1 FROM sys.schemas WHERE name = :n\"), {\"n\": SCHEMA}\n",
    "    ).first()\n",
    "    if not exists:\n",
    "        conn.exec_driver_sql(f\"CREATE SCHEMA [{SCHEMA}]\")\n",
    "    # test rapide\n",
    "    conn.execute(text(\"SELECT 1\"))\n",
    "\n",
    "# ========= Fonctions utils =========\n",
    "def table_name_from_path(path: str) -> str:\n",
    "    stem = os.path.splitext(os.path.basename(path))[0]\n",
    "    stem = stem.strip().lower().replace(\"-\", \"_\").replace(\" \", \"_\")\n",
    "    return NAME_MAP.get(stem, stem)  # fallback = nom normalisé\n",
    "\n",
    "# ========= Import CSV -> staging.<table> =========\n",
    "csv_files = sorted(glob.glob(os.path.join(CSV_DIR, \"*.csv\")))\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"Aucun CSV trouvé dans {CSV_DIR}\")\n",
    "\n",
    "for csv_path in csv_files:\n",
    "    table = table_name_from_path(csv_path)\n",
    "    print(f\"--> {os.path.basename(csv_path)}  -> [{SCHEMA}.{table}]\")\n",
    "\n",
    "    df = pd.read_csv(csv_path, low_memory=False)\n",
    "    df.columns = [c.strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
    "\n",
    "    # Limite SQL Server: 2100 paramètres par requête\n",
    "    # On choisit un chunksize sûr: floor(2100 / nb_colonnes)\n",
    "    ncols = max(len(df.columns), 1)\n",
    "    safe_chunk = max(1, min(10000, 2100 // ncols))\n",
    "\n",
    "    df.to_sql(\n",
    "        name=table,\n",
    "        con=engine,\n",
    "        schema=SCHEMA,\n",
    "        if_exists=\"replace\",     # \"append\" pour ajouter\n",
    "        index=False,\n",
    "        chunksize=safe_chunk,    # OK avec fast_executemany\n",
    "        method=None,             # pas de multi-VALUES => évite la limite 2100\n",
    "    )\n",
    "    print(f\"    OK ({len(df):,} lignes, chunksize={safe_chunk})\")\n",
    "\n",
    "print(\"✅ Import terminé dans le schéma 'staging' de la base olist_staging.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olist_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
